---
title: "**realPSD**: Theory and Design Principles"
author: "Martin Lysy, Feiyu Zhu"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{realPSD: Theory and Design Principles}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\tx}[1]{\textrm{#1}}
\newcommand{\XX}{{\bm{X}}}
\newcommand{\YY}{{\bm{Y}}}
\newcommand{\ZZ}{{\bm{Z}}}
\newcommand{\tth}{{\bm{\theta}}}
\newcommand{\pph}{{\bm{\varphi}}}
\newcommand{\aal}{{\bm{\alpha}}}
\newcommand{\bbe}{{\bm{\beta}}}
\newcommand{\s}{\sigma}
\newcommand{\f}{f}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\psd}{\operatorname{\mathcal S}}
\newcommand{\cov}{\operatorname{cov}}
\newcommand{\var}{\operatorname{var}}
\newcommand{\acf}{\operatorname{\gamma}}
\newcommand{\carfima}{\operatorname{CARFIMA}}
\newcommand{\rv}[3]{#2_{#1},\ldots,#2_{#3}}
\newcommand{\fs}{\f_s}
\newcommand{\kbt}{k_BT}
\newcommand{\Aw}{A_{\tx{w}}}
\newcommand{\Rw}{R_{\tx{w}}}

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  collapse = TRUE,
  comment = "#>"
)
# link to packages
pkg_link <- function(pkg, link) {
  if(link == "github") {
    link <- paste0("https://github.com/mlysy/", pkg)
  } else if(link == "cran") {
    link <- paste0("https://CRAN.R-project.org/package=", pkg)
  }
  paste0("[**", pkg, "**](", link, ")")
}
cran_link <- function(pkg) pkg_link(pkg, "cran")
github_link <- function(pkg) pkg_link(pkg, "github")
```

This vignette explains the theory behind the built-in estimation methods provided by **realPSD** and also discusses some technical details in designing **realPSD**. This package is essentially a port of `r github_link("realSHO")` whic is based on [MATLAB](https://www.mathworks.com/products/matlab.html), but with the option of freely adding one's own models and carrying out accurate and efficient gradient calculations.  The interface is through the R `r cran_link("TMB")` package, which uses [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD) to enable gradient-based optimization algorithms to efficiently perform parameter estimation.

For a complete exposition of the theory (with theorems and proofs) as well as the real-world application background, please check out this [preprint](). 

# Overview

Let $X(t)$ denote a continuous time mean-zero stationary process with autocorrelation $\acf(t) = \cov(X_s, X_{s+t})$. with power spectral density (PSD) defined via the inverse Fourier transform relation
$$
\acf(t) = \int_{-\infty}^\infty e^{-2\pi i t\f} \psd(\f) \, \ud \f,
$$
where $\f \in \mathbb R$ is the frequency in Hertz (Hz).  The **realPSD** package provides tools for estimation of parametric PSDs of the form
$$
\psd(\f,\tth) = \s^2 \cdot U(\f, \pph),
$$
where the PSD parameters are $\tth = (\s, \pph)$.   The data $\XX = (\rv 0 X {N-1})$ consist of discrete observations of $X(t)$ recorded at sampling frequency $\fs$, such that $X_n = X(n/\fs)$.

Traditional theory of time series always assumes that the time interval of the collected data $\XX = (\rv 0 X {N-1})$ is fixed and the asymptotics of the estimators are all about the situation when $N \to \infty$. However, in many modern scientific experiments, for example recordings collected by an [Atomic Force Microscope](https://en.wikipedia.org/wiki/Atomic_force_microscopy), **high-throughput (HTP) data** are very common. That is, we can let both $\fs \to \infty$ and $N \to \infty$. In this situation, traditional theory fails. To overcome this, traditional techniques have to be generalized. **realPSD** was built as a robust and efficient toolbox to realize this. That being said, you can still use **realPSD** for traditional settings, for example to fit an Ornstein-Uhlenbeck (OU) model, as we've already shown in the [quick tutorial](http://htmlpreview.github.io/?https://github.com/mlysy/realPSD/blob/devel-ferris-prerelease/doc/realPSD-quicktut.html).

<!-- This interface is written in Python, and uses [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD) to use gradient-based optimization algorithms to efficiently perform parameter estimation. -->

<!-- ~~Currently the interface is through the R `r cran_link("TMB")` package, which makes the underlying C++ code difficult to interface from a different language (e.g., Python, Matlab).  A later step is to create a self-contained C++ library relying only on `r pkg_link("CppAD", "https://coin-or.github.io/CppAD/doc/cppad.htm")`, the C++ automated differentiation (AD) library on top of which **TMB** is built.~~ -->

# Methods

**realPSD** provides three different methods for parameter estimation outlined below.  For each method, estimation is via optimization over a function of $\pph$, with the fitted value of $\s$ being profiled out.  In all cases the estimators are built on the discrete Fourier transform of the data, $\YY = (\rv 1 Y K)$, where $N = 2K + 1$ and

$$
Y_k = \frac{1}{N \fs} \left\vert \sum_{n=0}^{N-1} e^{-2\pi i k n} X_n\right\vert^2.
$$

**realPSD** provides a function `periodogram()` to calculate this $Y_k$. Note that the returned periodogram is just one-sided. For more details about how $Y_k$ is related to the Whittle-type loglikelihood in the MLE, please refer to Proposition 1 and the discussion thereafter in this [preprint]().

All the following methods are provided in the C++ header files in **realPSD**, please check out the source code of `FitMethods.hpp`, `MLEFit.hpp`, `NLSFit.hpp` and `LPFit.hpp` etc.

## Maximum Likelihood

The maximum likelihood estimator is obtained by maximizing the so-called Whittle loglikelihood function,
\begin{equation}
\label{eq:mle}
\ell_W(\tth \mid \YY) = - \sum_{k=1}^K\left(\frac{Y_k}{\psd_k(\tth)} + \log \psd_k(\tth)\right),
\end{equation}
where where $\psd_k(\tth) = \psd(\f_k, \tth)$ and $f_k = \tfrac k N \fs$.  Note that for given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:mle} is
$$
\hat \s^2(\pph) = \frac 1 K \sum_{k=1}^K \frac{Y_k}{U_k(\pph)},
$$
where $U_k(\pph) = U(\f_k, \pph)$.  Therefore, upon maximizing the profile likelihood
$$
\begin{aligned}
\ell_{W,\tx{prof}}(\pph \mid \YY) & = \ell_W(\pph, \hat \s(\pph) \mid \YY) \\
& = - K (1 + \log \hat \sigma^2(\pph)) - \sum_{k=1}^K \log U_k(\pph),
\end{aligned}
$$
we obtain the MLE $\hat \tth = (\hat \pph, \hat \s(\hat \pph))$.

## Nonlinear Least-Squares

In many situations with high-throughput (HTP) data, $K$ is very large, such that calculating the MLE becomes very computationally intensive.  An approximation to the MLE involves binning the periodogram ordinates.  That is, let $K = B \cdot N_B$, and
$$
\bar Y_m = \frac 1 B \sum_{k \in I_m} Y_k, \qquad I_m = \{k \in \mathbb N: (m-1)B < k \le mB\}.
$$
Then the nonlinear least-squares (NLS) estimator maximizes the objective function
\begin{equation}
\label{eq:nls}
Q_{\tx{NLS}}(\tth) = - \sum_{m=1}^{N_B} \big(\bar Y_m - \bar{\psd}_m(\tth)\big)^2,
\end{equation}
where $\bar{\psd}_m(\tth) = \psd(\bar \f_m, \tth)$ and $\bar \f_m = \tfrac 1 B \sum_{k\in I_m} \f_k$.  For given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:nls} is
$$
\hat \s^2(\pph) = \frac{\sum_{m=1}^{N_B} \bar{U}_m(\pph) \bar Y_m }{\sum_{m=1}^{N_B} \bar{U}_m(\pph)^2},
$$
where $\bar{U}_m(\pph) = U(\bar f_m, \pph)$.  Therefore, upon maximizing the objective function
$$
Q_{\tx{NLS},\tx{prof}}(\pph) = -\sum_{m=1}^{N_B} \big(\bar Y_m - \hat \sigma^2(\pph) \cdot \bar{U}_m(\pph)\big)^2,
$$
we obtain the maximum of \\eqref{eq:nls} via $\hat \tth = (\hat \pph, \hat \sigma(\hat \pph))$.

## Log-Periodogram (Variance-Stabilized Least-Squares)

Let $Z_m = \log(\bar Y_m)$.  Then the log-periodogram (LP) estimator maximizes the approximate loglikelihood
\begin{equation}
\label{eq:lp}
\ell_{\tx{LP}}(\tth \mid \YY) = -\frac B 2 \sum_{m=1}^{N_B} \big(Z_m + C_B - \log \bar{\psd}_m(\tth)\big)^2,
\end{equation}
where $C_B = \log B - \psi(B)$ and $\psi(x)$ is the [digamma function](https://en.wikipedia.org/wiki/Digamma_function).  For given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:lp} is
$$
\hat \s^2(\pph) = \exp\left\{\hat \zeta(\pph) + C_B\right\}, \qquad \hat \zeta(\pph) = \frac{1}{N_B} \sum_{m=1}^{N_B} Z_m - \log \bar{U}_m(\pph),
$$
such that upon maximizing
$$
\ell_{\tx{LP},\tx{prof}}(\pph \mid \YY) = - \sum_{m=1}^{N_B} \big(Z_m - \hat \zeta(\pph) - \log \bar U_m(\pph)\big)^2
$$
we obtain the maximum of \\eqref{eq:lp} via $\hat \tth = (\hat \pph, \hat \sigma(\hat \pph))$.

## Variance Estimators

The MLE loglikelihood \\eqref{eq:mle} can be used to calculate the observed Fisher information, which in turn can be used to produce the variance estimates
$$
\widehat{\var}(\hat \tth_{\tx{MLE}}) = -\left[\frac{\partial^2}{\partial \tth^2} \ell_W(\hat\tth_{\tx{MLE}} \mid \YY)\right]^{-1}.
$$
It turns out that the same holds for the LP loglikelihood \\eqref{eq:lp} up to a small inflation factor:
$$
\widehat{\var}(\hat \tth_{\tx{LP}}) = D_B \times -\left[\frac{\partial^2}{\partial \tth^2} \ell_{\tx{LP}}(\hat\tth_{\tx{LP}} \mid \YY)\right]^{-1},
$$
where $D_B = B \psi'(B)$.  Note that $D_{10} = `r round(10 * trigamma(10), 2)`$ and $D_{100} = `r round(100 * trigamma(100), 4)`$, such that correcting by $D_B$ has little impact in practice.

<!-- Both the MLE and LP loglikelihoods \\eqref{eq:mle} and \\eqref{eq:lp} can be used to calculate the observed Fisher information, which in turn can be used to produce the variance estimates -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \widehat{\var}(\hat \tth_{\tx{MLE}}) & = -\left[\frac{\partial^2}{\partial \tth^2} \ell_W(\hat\tth_{\tx{MLE}} \mid \YY)\right]^{-1} \\ -->
<!-- \widehat{\var}(\hat \tth_{\tx{LP}}) & = -\left[\frac{\partial^2}{\partial \tth^2} \ell_{\tx{LP}}(\hat\tth_{\tx{LP}} \mid \YY)\right]^{-1}. -->
<!-- \end{aligned} -->
<!-- $$ -->

In contrast to the likelihood-based estimators, the variance of the NLS estimator is defined by the sandwich formula
\begin{equation}
\label{eq:sand}
\widehat{\var}(\hat \tth_{\tx{NLS}}) = - \bm A^{-1} \bm B \bm A^{-1},
\end{equation}
where for $g_m(\tth) = (\bar Y_m - \bar{\psd}_m(\tth))^2$, we have
$$
\begin{aligned}
\bm A & = \sum_{m=1}^{N_B} \frac{\partial^2}{\partial \tth^2} g_m(\hat \tth_{\tx{NLS}}) \\
\bm B & = \sum_{m=1}^{N_B} \left[\frac{\partial}{\partial \tth} g_m(\hat \tth_{\tx{NLS}})\right]\left[\frac{\partial}{\partial \tth} g_m(\hat \tth_{\tx{NLS}})\right]'.
\end{aligned}
$$

## Model Residuals

Both the NLS and LP (profile) optimization problems can be rewritten as minimizing a sum-of-squares objective function
$$
S(\pph) = \sum_{m=1}^{N_B} r_m(\pph)^2,
$$
for which specialized optimization algorithms such as [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) or [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) can be used.

## Scaling

For numerical stability, we have found that it is often preferable to express the Whittle likelihood \\eqref{eq:mle} as
$$
\ell_W(\tth \mid \YY) = - \sum_{k=1}^K\left(\frac{Y_k^{(s)}}{\psd_k(\tth)/C_s} + \log \psd_k(\tth)\right),
$$
where $Y_k^{(s)} = Y_k/C_s$ and $C_s > 0$ is a scaling factor (e.g., $C_s = K^{-1} \sum_{k=1}^K Y_k$).  In this case, the value of $\sigma^2$ maximing \\eqref{eq:mle} for given $\pph$ is
$$
\hat \s^2(\pph) = C_s \cdot \hat \s^2_{(s)}(\pph), \qquad \hat \s^2_{(s)}(\pph) = \frac {1} K \sum_{k=1}^K \frac{Y_k^{(s)}}{U_k(\pph)},
$$
and the profile loglikelihood may be expressed as
$$
\ell_{W,\tx{prof}}(\pph \mid \YY) = - K (1 + \log C_s + \log \hat \sigma_{(s)}^2(\pph)) - \sum_{k=1}^K \log U_k(\pph).
$$

Similarly, for the NLS estimator we may maximize the objective function
$$
Q_{\tx{NLS}}^{(s)}(\tth) = - \sum_{m=1}^{N_B} \big(\bar Y_m^{(s)} - \bar{\psd}_m(\tth)/C_s\big)^2,
$$
where $\bar Y_m^{(s)} = \bar Y_m / C_s$.  The corresponding profiled function is
$$
Q_{\tx{NLS},\tx{prof}}^{(s)}(\pph) = -\sum_{m=1}^{N_B} \big(\bar Y_m^{(s)} - \hat \sigma_{(s)}^2(\pph) \cdot \bar{U}_m(\pph)\big)^2,
$$
where
$$
\hat \s_{(s)}^2(\pph) = \frac{\sum_{m=1}^{N_B} \bar{U}_m(\pph) \bar Y_m^{(s)} }{\sum_{m=1}^{N_B} \bar{U}_m(\pph)^2} = \hat \s^2(\pph)/C_s.
$$

Scaling is typically not needed for the LP estimator, but for completeness we provide the calculations here.  That is, for $Z_m^{(s)} = Z_m - C_s$, we have
$$
\ell_{\tx{LP}}(\tth \mid \YY) = - \frac B 2 \big(Z_m^{(s)} + C_s + C_B - \log \bar{\psd}_m(\tth) \big)^2
$$
and
$$
\hat \s^2(\pph) = \exp\left\{\hat \zeta_{(s)}(\pph) + C_s + C_B\right\}, \qquad \hat \zeta_{(s)}(\pph) = \frac{1}{N_B} \sum_{m=1}^{N_B} Z_m^{(s)} - \log \bar{U}_m(\pph).
$$

## Median Binning

The NLS and LP estimators are defined in terms of the bin mean $\bar Y_m$.  To decrease sensitivity to outliers, one might consider the bin median $\tilde Y_m = \operatorname{median}\{Y_k: k \in I_m\}$.  In this case, we have
$$
\begin{aligned}
\lim_{B\to\infty} E[\tilde Y_m] & = \bar{\psd}_m(\tth) \cdot \log 2, \\ \lim_{B\to\infty} E[\log \tilde Y_m] & = \log \bar{\psd}_m(\tth) + \log \log 2,
\end{aligned}
$$
such that we may try to maximize the objective functions
$$
\begin{aligned}
\tilde Q^{(s)}_{\tx{NLS}}(\tth) & = - \sum_{m=1}^{N_B}\big(\tilde Y_m^{(s)} - (\log 2/C_s) \cdot \bar{\psd}_m(\tth)\big)^2 \\
\tilde Q^{(s)}_{\tx{LP}}(\tth) & = - \sum_{m=1}^{N_B} \big(\tilde Z_m^{(s)} + C_s - \log \log 2 - \log \bar{\psd}_m(\tth)\big)^2.
\end{aligned}
$$
Note here that neither the LP nor NLS estimators are asympotically loglikelihoods, such that variance estimators should use the sandwich formula \\eqref{eq:sand}.

## List of Built-in Fitting Methods

For general users, the fitting methods specified in `FitMethods.hpp` are hidden. You don't really need to know the underlying details, since **realPSD** has already handled (by using R6 class) choosing the proper fitting methods for you. But for adavanced users who are insterested in using `TMB::MakeADFun` directly, one can specify `method` as

 - `UFun`: evaluate U function $U(\f, \pph)$.
 - `MLE_ufun`: evaluate U function $U_k(\pph)$ for MLE method.
 - `MLE_nll`: evaluate the negative log-likelihood function with proper scaling for MLE method.
 - `MLE_nlp`: evaluate the profile likelihood with proper scaling for MLE method.
 - `MLE_zeta`: evaluate $\sigma^2$ with proper scaling for MLE method.
 - `MLE_res`: get the vector of residuals for MLE method.

 - `LP_ufun`: evaluate $\bar{U}_m(\pph)$ for LP method.
 - `LP_nll`: evaluate the negative log-likelihood function with proper scaling for LP method. 
 - `LP_nlp`: evaluate the profile likelihood with proper scaling for LP method.
 - `LP_zeta`: evaluate $\zeta$ with proper scaling for LP method.
 - `LP_res`: get the vector of residuals for LP method.

 - `NLS_ufun`: evaluate U function $U_k(\pph)$ for NLS method.
 - `NLS_nll`: evaluate the negative log-likelihood function with proper scaling for NLS method.
 - `NLS_nlp`: evaluate the profile likelihood with proper scaling for NLS method.
 - `NLS_zeta`: evaluate $\sigma^2$ with proper scaling for NLS method.
 - `NLS_res`: get the vector of residuals for NLS method.


# Examples

## Simple Harmonic Oscillator with Noise Floor

The SHOW model is given by
\begin{equation}
\label{eq:show}
\begin{aligned}
\psd(\f, \tth) & = \Aw + \frac{\kbt/(k \cdot \pi \f_0 Q)}{[(\f/\f_0)^2-1]^2 + [f/(\f_0Q)]^2} \\
& = \sigma^2 \times \left\{\Rw + \frac{1}{[(\f/\f_0)^2-1]^2 + [f/\gamma]^2}\right\},
\end{aligned}
\end{equation}
where $\sigma^2 = \kbt/(k\cdot \pi \f_0Q)$, $\Rw = \Aw/\sigma^2$, $\gamma = \f_0Q$, and $\pph = (\f_0, \gamma, \Rw)$.

This is a built-in model in **realPSD** with three different parameterizations:

  - natural parameterization as shown above;
  - log-transformed parameters, i.e. $\pph = (\log\f_0, \log Q, \log \Rw)$;
  - computational parameterization, i.e. $\pph = (\f_0, \gamma, \Rw)$ where $\gamma = \f_0 Q$.

To call this model using R6 class from R, one can use the following template.

```{r, SHOW_R6, eval = FALSE}
model <- sample(c("SHOW_nat", "SHOW_log", "SHOW_comp"), size = 1) # can be chosen from any of these
your_model <- R6::R6Class(
          classname = "your_model",
          inherit = realPSD::psd_model,
          private = list(
            Temp_ = NULL, # temperature
            n_phi_ = 3, # specify the number of parameters, for SHOW model, f0, Q, Rw
            tmb_DLL_ = "realPSD_TMBExports",
            tmb_model_ = model
          ),
          active = list(
            #' @field Temp Temperature of the system.
            Temp = function() private$Temp_
          )
        )
```

Then if one calls `your_model$ufun()`, it actually returns a `TMB::MakeADFun()` object. For those who are familiar with **TMB**, you can also directly call `TMB::MakeADFun` as follows (say we want to get the U function under MLE):

```{r, SHOW_MLE_ufun, eval = FALSE}
# get MLE Ufun
scale <- mean(Ypsd)
# create TMB model object
tmod <- TMB::MakeADFun(data = list(model = model,
                                   method = "MLE_ufun",
                                   f = matrix(freq),
                                   Y = matrix(Ypsd/scale),
                                   scale = log(scale)),
                       parameters = list(phi = matrix(rep(0, 3))),
                       ADreport = TRUE,
                       silent = TRUE, DLL = "realPSD_TMBExports")
```

## Fractional Ornstein-Uhlenbeck Model

The fOU model is expressed as
$$
\ud X(t) = -\gamma X(t)\, \ud t + \tau \, \ud B_t^H,
$$
where $\gamma, \tau > 0$ and $B_t^H$ is fractional Brownian motion with Hurst parameter $0 < H < 1$.  The PSD for this model is
\begin{equation}
\label{eq:fcar}
\psd(\f, \tth) = \sigma^2 \times \frac{|\f|^{1-2H}}{\f^2 + \gamma^2},
\end{equation}
where $\pph = (H, \gamma)$ and $\sigma^2 = \tau^2 \cdot \Gamma(2H+1)\sin(\pi H)/(2\pi)$.

## CARFIMA Models

More generally, a $\carfima(p,q)$ process is defined by the differential equation
$$
X_t^{(p)} = + \sum_{k=1}^{p-1} \alpha_k X_t^{(k-1)} + \tau \left[B_t^{H(1)} + \sum_{m=1}^{q} \beta_m B_t^{H(m+1)}\right],
$$
where $X(t)$ is stationary if and only if
$$
\alpha(x) = x^p - \sum_{k=1}^{p-1} \alpha_k x^{k-1} = \prod_{k=1}^p(x - r_k)
$$
is such that $\mathscr R(r_k) < 0$ for $k = 1,\ldots,p$. The PSD of the model is given by
\begin{equation}
\label{eq:carfima}
\psd(\f, \tth) = \sigma^2 \times \frac{|\f|^{1-2H} \cdot |1 + \sum_{m=1}^q \beta_m (i\f)^m|^2}{|(i\f)^p - \sum_{k=1}^p \alpha_k (i\f)^{k-1}|^2}, 
\end{equation}
where $\pph = (H, \aal, \bbe)$, and $\sigma^2 = \tau^2 \cdot \Gamma(2H+1)\sin(\pi H)/(2\pi)$.  
<!-- , $0 < H < 1$, and $\aal = (\rv 1 \alpha q )$ and $\bbe = (\rv 1 \beta p )$ are such that the complex polynomials in the numerator and denominator of \\eqref{eq:carfima} have no roots in the unit circle.  The model parameters are $\pph = (\aal, \bbe, H)$. -->
Since **TMB** does not support complex arithmetic, the polynomials are calculated as follows.

Consider a polynomial $p(x) = \sum_{k=0}^p \alpha_k x^k$ with $\alpha_k \in \mathbb R$.
\begin{align*}
p(i\f) 
	& = \sum_{k=0}^p \alpha_k (i\f)^k \\
	& = \sum_{k=0}^{q} \alpha_{2k} (i\f)^{2k} + \sum_{k=0}^r \alpha_{2k+1} (i\f)^{2k+1} \\
	& = \sum_{k=0}^q (-1)^k \alpha_{2k} f^{2k} + i\f \sum_{k=0}^r (-1)^k \alpha_{2k+1} f^{2k}
\end{align*}


# Interfacing with **TMB**

`r cran_link("TMB")` is a great interface for R users to make use of the [**CppAD**](https://coin-or.github.io/CppAD/doc/cppad.htm) C++ library for [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). However, the study curve of **TMB** may be steep, especially for those new to this area. When **realPSD** was designed, we would like to make it as easy as possible for **realPSD** users to realize their models without dealing with **TMB** directly. The principle and coding practices are well explained [here](https://github.com/mlysy/TMBtools#tmb-coding-practices). It is strongly recommended to read the documentation in `r github_link("TMBtools")` and the instructions [here](https://github.com/kaskr/adcomp/wiki/Development#distributing-code) for those who are interested in using **TMB** to build flexible statistical models in R.

## External Constructor for User-defined Model Object

**TMB** and other source code (e.g., [**Rcpp**]((http://www.rcpp.org/))) in an R package cannot be merged into a single shared library, as documented [here](https://github.com/kaskr/adcomp/issues/247).  Therefore, the approach taken by **realPSD** is to have a separate shared object for the **TMB** models

If you still remember the OU example in the [quick tutorial](http://htmlpreview.github.io/?https://github.com/mlysy/realPSD/blob/devel-ferris-prerelease/doc/realPSD-quicktut.html), one of the necessary steps in writing your own model C++ header file is to include an external constructor like this

```C++
#undef TMB_OBJECTIVE_PTR
#define TMB_OBJECTIVE_PTR obj
  /// External constructor for `ou::UFun` objects.
  ///
  /// The arguments to this function are always `N` and `obj`.  Inside the function, we can specify additional TMB macros (`DATA_VECTOR`, etc.), to obtain inputs to the `UFun` constructor.
  ///
  /// @param[in] N Number of frequency/psd observations.
  /// @param[in] obj Pointer to the TMB object.
  /// @return An `ou::UFun` object.
  template<class Type>
  UFun<Type> make_Ufun(int N, objective_function<Type>* obj) {
    return UFun<Type>(N);
  }
#undef TMB_OBJECTIVE_PTR
#define TMB_OBJECTIVE_PTR this
```

This is a workaround to let the TMB object pointer find the current model object (which is not built into the **realPSD** package).

## Compile-on-the-fly

To compile any user-defined model object on-the-fly, we make use of [whisker](https://github.com/edwindj/whisker) to automatically create a template C++ file which is needed for **TMB** to compile the model correctly. The template was designed in a similar way as proposed by `r github_link("TMBtools")`.

This technique has already been included into the built-in function `realPSD::make_psd_model`. Thus, users do not need to worry about how to realize this themselves. As we've already used in the OU example, by calling `make_psd_model` as follows

```{r, ou_make_psd_model, eval = FALSE}
# create c++ file
ou_cpp <- make_psd_model(model = "OU",
                         header = "OU_Model.hpp",
                         class = "ou::UFun",
                         ctor = "ou::make_Ufun",
                         standalone = TRUE)
```

**realPSD** automatically generate a `.cpp` file under the current working directory (named `OU_FitMethods.cpp`) which contains the following content.

```{r ou_ufun, echo = FALSE, results = "asis"}
cat("```cpp", readLines("OU_FitMethods.cpp"), "```", sep = "\n")
```

This template allows users to put their models in separate header files from each other and compile the model using **TMB** correctly. 

## Wrap the Model Object by R6 Class

To let users to directly deal with **TMB** or the `FitMethods.hpp` in **realPSD** is not a wise choice and is very clumsy. Instead, we wrap the model object in an R6 class, which greatly streamlines the workflow of building and estimating process. Any user-defined PSD models will be a inherited class from the built-in base class `realPSD::psd_model` in which we have already defined all the necessary infrastructures. Therefore, fitting any user-defined PSD models using **realPSD** is a very consistent process. If you are insterested in how we realize this, please check out the source code of `psd_model.R`. For more details about R6 class, one can see [here](https://r6.r-lib.org/index.html).

<!-- where $g_m(\tth) = (\bar Y_m - \bar{\psd}_m(\tth))^2$. -->

<!-- ## Implementation -->

<!-- ### Python -->

<!-- I'm not sure about OOP in Python, but in pseudo C++ I suggest we have a base class for each PSD model defined by $U(\f, \pph)$, and derived class for each method.  So for example: -->

<!-- ```{Rcpp, eval = FALSE} -->
<!-- class showModel { -->
<!-- // private/protected members here -->
<!-- public: -->
<!--   double UFun(const double f, const Vector& phi);   -->
<!-- }; -->

<!-- template <class Model> -->
<!-- class LPFit : public Model { -->
<!-- public: -->
<!--   // constructor: copy + allocate memory here -->
<!--   LPFit(const Vector& xPSD, const Vector& yPSD, const int binSize); -->
<!--   // conditional optimum of log_tau = log(tau) = log(sigma^2) -->
<!--   double logTauFit(const Vector& phi);		  -->
<!--   // objective function: set up to minimize it -->
<!--   double obj(const Vector& phi); -->
<!--   // gradient & hessian -->
<!--   void grad(Vector& g, const Vector& phi); -->
<!--   void hess(Matrix& H, const Vector& phi); -->
<!--   // sometimes these re-use calculations from OFun. -->
<!--   // In that case, it's more efficient to have something like -->
<!--   double obj(Vector& g, Matrix& H, const Vector& phi); -->
<!--   // default optimization method. -->
<!--   //user has the option to do this manually with `obj`, `grad`, `hess` if -->
<!--   // default doesn't work. -->
<!--   // Or, simply don't provide a default if it's too complicated! -->
<!--   void fit(Vector& phi, double& logTau, const Vector& phiInit, -->
<!--            ...); // ellipsis is for additional tuning parameters -->
<!--   // variance estimator -->
<!--   void var(Matrix& V, const Vector& phi, const double logTau); -->
<!-- }; -->
<!-- ``` -->

<!-- Some useful Python packages: -->

<!-- - [**autograd**](https://github.com/HIPS/autograd): Automatic differentiation of Python and Numpy functions. -->
<!-- - [**NLopt**](https://nlopt.readthedocs.io/en/latest/): Excellent nonlinear optimization library, with Python interface.  I suggest using gradient-based algorithms in combination with the above. -->

<!-- Other functions the library should contain: -->

<!-- - `tsSim`: Simulate time series with given PSD. -->
<!-- - `periodogram`: Calculate the periodogram of a time series. -->
<!-- - `fisherGstat`: For denoising. -->
