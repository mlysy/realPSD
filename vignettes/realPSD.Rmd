---
title: "**realPSD**: Development Notes"
author: "Martin Lysy, Feiyu Zhu"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{realPSD Development Notes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\tx}[1]{\textrm{#1}}
\newcommand{\XX}{{\bm{X}}}
\newcommand{\YY}{{\bm{Y}}}
\newcommand{\ZZ}{{\bm{Z}}}
\newcommand{\tth}{{\bm{\theta}}}
\newcommand{\pph}{{\bm{\varphi}}}
\newcommand{\aal}{{\bm{\alpha}}}
\newcommand{\bbe}{{\bm{\beta}}}
\newcommand{\s}{\sigma}
\newcommand{\f}{f}
\newcommand{\psd}{\operatorname{\mathcal S}}
\newcommand{\cov}{\operatorname{cov}}
\newcommand{\var}{\operatorname{var}}
\newcommand{\acf}{\operatorname{\gamma}}
\newcommand{\carfima}{\operatorname{CARFIMA}}
\newcommand{\rv}[3]{#2_{#1},\ldots,#2_{#3}}
\newcommand{\fs}{\f_s}
\newcommand{\kbt}{k_BT}
\newcommand{\Aw}{A_{\tx{w}}}
\newcommand{\Rw}{R_{\tx{w}}}

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
# link to packages
pkg_link <- function(pkg, link) {
  if(link == "github") {
    link <- paste0("https://github.com/mlysy/", pkg)
  } else if(link == "cran") {
    link <- paste0("https://CRAN.R-project.org/package=", pkg)
  }
  paste0("[**", pkg, "**](", link, ")")
}
cran_link <- function(pkg) pkg_link(pkg, "cran")
github_link <- function(pkg) pkg_link(pkg, "github")
```

## Overview

Let $X(t)$ denote a continuous time mean-zero stationary process with autocorrelation $\acf(t) = \cov(X_s, X_{s+t})$. with power spectral density (PSD) defined via the inverse Fourier transform relation
$$
\acf(t) = \int_{-\infty}^\infty e^{-2\pi i t\f} \psd(\f) \, \mathrm{d} \f,
$$
where $\f \in \mathbb R$ is the frequency in Hertz.  The **realPSD** package provides tools for estimation of parametric PSDs of the form
$$
\psd(\f,\tth) = \s^2 \cdot U(\f, \pph),
$$
where the PSD parameters are $\tth = (\s, \pph)$.   The data $\XX = (\rv 0 X {N-1})$ consist of discrete observations of $X(t)$ recorded at sampling frequency $\fs$, such that $X_n = X(n/\fs)$.  

This package is essentially a port of `r github_link("realSHO")`, but with the option of adding one's own models.  The interface is through the R `r cran_link("TMB")` package, which uses [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD) to enable gradient-based optimization algorithms to efficiently perform parameter estimation.


<!-- This interface is written in Python, and uses [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD) to use gradient-based optimization algorithms to efficiently perform parameter estimation. -->

<!-- ~~Currently the interface is through the R `r cran_link("TMB")` package, which makes the underlying C++ code difficult to interface from a different language (e.g., Python, Matlab).  A later step is to create a self-contained C++ library relying only on `r pkg_link("CppAD", "https://coin-or.github.io/CppAD/doc/cppad.htm")`, the C++ automated differentiation (AD) library on top of which **TMB** is built.~~ -->

## Methods

**realPSD** provides three different methods for parameter estimation outlined below.  For each method, estimation is via optimization over a function of $\pph$, with the fitted value of $\s$ being profiled out.  In all cases the estimators are built on the discrete Fourier transform of the data, $\YY = (\rv 1 Y K)$, where $N = 2K + 1$ and
$$
Y_k = \frac 1 N \left\vert \sum_{n=0}^{N-1} e^{-2\pi i k n} X_n\right\vert^2.
$$

### Maximum Likelihood

The maximum likelihood estimator is obtained my maximizing the so-called Whittle loglikelihood function,
\begin{equation}
\label{eq:mle}
\ell_W(\tth \mid \YY) = - \sum_{k=1}^K\left(\frac{Y_k}{\psd_k(\tth)} + \log \psd_k(\tth)\right),
\end{equation}
where $\psd_k(\tth) = \fs \cdot \psd(\f_k, \tth)$ and $f_k = \tfrac k N \fs$.  Note that for given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:mle} is
$$
\hat \s^2(\pph) = \frac 1 K \sum_{k=1}^K \frac{Y_k}{U_k(\pph)},
$$
where $U_k(\pph) = \fs \cdot U(\f_k, \pph)$.  Therefore, upon maximizing the profile likelihood
$$
\begin{aligned}
\ell_{W,\tx{prof}}(\pph \mid \YY) & = \ell_W(\pph, \hat \s(\pph) \mid \YY) \\
& = - K (1 + \log \sigma^2(\pph)) - \sum_{k=1}^K \log U_k(\pph),
\end{aligned}
$$
we obtain the MLE $\hat \tth = (\hat \pph, \hat \s(\hat \pph))$.

### Nonlinear Least-Squares

In many situations with high-throughput (HTP) data, $K$ is very large, such that calculating the MLE becomes very computationally intensive.  An approximation to the MLE involves binning the periodogram ordinates.  That is, let $K = B \cdot N_B$, and
$$
\bar Y_m = \frac 1 B \sum_{k \in I_m} Y_k, \qquad I_m = \{k \in \mathbb N: (m-1)B < k \le mB\}.
$$
Then the nonlinear least-squares (NLS) estimator maximizes the objective function
\begin{equation}
\label{eq:nls}
Q_{\tx{NLS}}(\tth) = - \sum_{m=1}^{N_B} \big(\bar Y_m - \bar{\psd}_m(\tth)\big)^2,
\end{equation}
where $\bar{\psd}_m(\tth) = \fs \cdot \psd(\bar \f_m, \tth)$ and $\bar \f_m = \tfrac 1 B \sum_{k\in I_m} \f_k$.  For given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:nls} is
$$
\hat \s^2(\pph) = \frac{\sum_{m=1}^{N_B} \bar{U}_m(\pph) \bar Y_m }{\sum_{m=1}^{N_B} \bar{U}_m(\pph)^2},
$$
where $\bar{U}_m(\pph) = \fs \cdot U(\bar f_m, \pph)$.  Therefore, upon maximizing the objective function
$$
Q_{\tx{NLS},\tx{prof}}(\pph) = -\sum_{m=1}^{N_B} \big(\bar Y_m - \hat \sigma^2(\pph) \cdot \bar{U}_m(\pph)\big)^2,
$$
we obtain the maximum of \\eqref{eq:nls} via $\hat \tth = (\hat \pph, \hat \sigma(\hat \pph))$.

### Variance-Stabilized Least-Squares

Let $Z_m = \log(\bar Y_m)$.  Then the log-periodogram (LP) estimator maximizes the approximate loglikelihood
\begin{equation}
\label{eq:lp}
\ell_{\tx{LP}}(\tth \mid \YY) = -\frac B 2 \sum_{m=1}^{N_B} \big(Z_m + C_B - \log \bar{\psd}_m(\tth)\big)^2,
\end{equation}
where $C_B = \log B - \psi(B)$ and $\psi(x)$ is the [digamma function](https://en.wikipedia.org/wiki/Digamma_function).  For given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:lp} is
$$
\hat \s^2(\pph) = \exp\left\{\hat \zeta(\pph) + C_B\right\}, \qquad \hat \zeta(\pph) = \frac{1}{N_B} \sum_{m=1}^{N_B} Z_m - \log \bar{U}_m(\pph),
$$
such that upon maximizing
$$
\ell_{\tx{LP},\tx{prof}}(\pph \mid \YY) = - \sum_{m=1}^{N_B} \big(Z_m - \hat \zeta(\pph) - \log \bar U_m(\pph)\big)^2
$$
we obtain the maximum of \\eqref{eq:lp} via $\hat \tth = (\hat \pph, \hat \sigma(\hat \pph))$.

### Variance Estimators

Both the MLE and LP loglikelihoods \\eqref{eq:mle} and \\eqref{eq:lp} can be used to calculate the observed Fisher information, which in turn can be used to produce the variance estimates
$$
\begin{aligned}
\widehat{\var}(\hat \tth_{\tx{MLE}}) & = -\left[\frac{\partial^2}{\partial \tth^2} \ell_W(\hat\tth_{\tx{MLE}} \mid \YY)\right]^{-1} \\
\widehat{\var}(\hat \tth_{\tx{LP}}) & = -\left[\frac{\partial^2}{\partial \tth^2} \ell_{\tx{LP}}(\hat\tth_{\tx{LP}} \mid \YY)\right]^{-1}.
\end{aligned}
$$
The variance of the NLS estimator is defined by the sandwich formula
$$
\widehat{\var}(\hat \tth_{\tx{NLS}}) = - \bm A^{-1} \bm B \bm A^{-1},
$$
where for $g_m(\tth) = (\bar Y_m - \bar{\psd}_m(\tth))^2$, we have
$$
\begin{aligned}
\bm A & = \sum_{m=1}^{N_B} \frac{\partial^2}{\partial \tth^2} g_m(\hat \tth_{\tx{NLS}}) \\
\bm B & = \sum_{m=1}^{N_B} \left[\frac{\partial}{\partial \tth} g_m(\hat \tth_{\tx{NLS}})\right]\left[\frac{\partial}{\partial \tth} g_m(\hat \tth_{\tx{NLS}})\right]'.
\end{aligned}
$$

### Model Residuals

Both the NLS and LP (profile) optimizaiton problems can be rewritten as minimizing a sum-of-squares objective function
$$
S(\pph) = \sum_{m=1}^{N_B} r_m(\pph)^2,
$$
for which specialized optimization algorithms such as [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) or [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) can be used.


<!-- where $g_m(\tth) = (\bar Y_m - \bar{\psd}_m(\tth))^2$. -->

<!-- ## Implementation -->

<!-- ### Python -->

<!-- I'm not sure about OOP in Python, but in pseudo C++ I suggest we have a base class for each PSD model defined by $U(\f, \pph)$, and derived class for each method.  So for example: -->

<!-- ```{Rcpp, eval = FALSE} -->
<!-- class showModel { -->
<!-- // private/protected members here -->
<!-- public: -->
<!--   double UFun(const double f, const Vector& phi);   -->
<!-- }; -->

<!-- template <class Model> -->
<!-- class LPFit : public Model { -->
<!-- public: -->
<!--   // constructor: copy + allocate memory here -->
<!--   LPFit(const Vector& xPSD, const Vector& yPSD, const int binSize); -->
<!--   // conditional optimum of log_tau = log(tau) = log(sigma^2) -->
<!--   double logTauFit(const Vector& phi);		  -->
<!--   // objective function: set up to minimize it -->
<!--   double obj(const Vector& phi); -->
<!--   // gradient & hessian -->
<!--   void grad(Vector& g, const Vector& phi); -->
<!--   void hess(Matrix& H, const Vector& phi); -->
<!--   // sometimes these re-use calculations from OFun. -->
<!--   // In that case, it's more efficient to have something like -->
<!--   double obj(Vector& g, Matrix& H, const Vector& phi); -->
<!--   // default optimization method. -->
<!--   //user has the option to do this manually with `obj`, `grad`, `hess` if -->
<!--   // default doesn't work. -->
<!--   // Or, simply don't provide a default if it's too complicated! -->
<!--   void fit(Vector& phi, double& logTau, const Vector& phiInit, -->
<!--            ...); // ellipsis is for additional tuning parameters -->
<!--   // variance estimator -->
<!--   void var(Matrix& V, const Vector& phi, const double logTau); -->
<!-- }; -->
<!-- ``` -->

<!-- Some useful Python packages: -->

<!-- - [**autograd**](https://github.com/HIPS/autograd): Automatic differentiation of Python and Numpy functions. -->
<!-- - [**NLopt**](https://nlopt.readthedocs.io/en/latest/): Excellent nonlinear optimization library, with Python interface.  I suggest using gradient-based algorithms in combination with the above. -->

<!-- Other functions the library should contain: -->

<!-- - `tsSim`: Simulate time series with given PSD. -->
<!-- - `periodogram`: Calculate the periodogram of a time series. -->
<!-- - `fisherGstat`: For denoising. -->


## Examples

### Simple Harmonic Oscillator with Noise Floor

The SHOW model is given by
\begin{equation}
\label{eq:show}
\begin{aligned}
\psd(\f, \tth) & = \Aw + \frac{\kbt/(k \cdot \pi \f_0 Q)}{[(\f/\f_0)^2-1]^2 + [f/(\f_0Q)]^2} \\
& = \sigma^2 \times \left\{\Rw + \frac{1}{[(\f/\f_0)^2-1]^2 + [f/\gamma]^2}\right\},
\end{aligned}
\end{equation}
where $\sigma^2 = \kbt/(k\cdot \pi \f_0Q)$, $\Rw = \Aw/\sigma^2$, $\gamma = \f_0Q$, and $\pph = (\f_0, \gamma, \Rw)$.

### CARFIMA Models

The PSD of a $\carfima(p,q)$ model is given by
\begin{equation}
\label{eq:carfima}
\psd(\f, \tth) = \sigma^2 \times \frac{|\f|^{1-2H} \cdot |1 + \sum_{k=1}^q \alpha_k (i\f)^k|^2}{|(i\f)^p - \sum_{m=1}^p \beta_m (i\f)^{m-1}|^2}, 
\end{equation}
where $\sigma^2 = \tau^2 \cdot \Gamma(2H+1)\sin(\pi H)/(2\pi)$, $0 < H < 1$, and $\aal = (\rv 1 \alpha q )$ and $\bbe = (\rv 1 \beta p )$ are such that the complex polynomials in the numerator and denominator of \\eqref{eq:carfima} have no roots in the unit circle.  The model parameters are $\pph = (\aal, \bbe, H)$.

As **TMB** does not support complex arithmetic, 

```{r}
# square norm of complex polynomial sum( alpha * (1i * x)^(0:d) )
# vectorize in argument x
sqr_poly <- function(alpha, x) {
  pol <- sapply(x, function(xx) sum( alpha * (1i * xx)^(1:length(alpha)-1) ))
  pol
  ## abs(pol)^2
}

# now the for-loop way
sqr_poly2 <- function(alpha, x) {
  deg <- length(alpha)-1 # degree
  polr <- poli <- 0 # real and imaginary parts
  sigr <- sigi <- 1 # real and imaginary signs
  x2 <- x * x # square of argument
  for(j in 0:deg) {
    if(j %% 2 == 0) {
      polr <- polr + sigr * alpha[j+1] * x^j
      sigr <- sigr * -1
    } else {
      poli <- poli + sigi * alpha[j+1] * x^j
      sigi <- sigi * -1
    }
  }
  polr + 1i * poli
}

deg <- 5
n <- 10
alpha <- rnorm(deg+1)
x <- rnorm(n)

sqr_poly(alpha, x) - sqr_poly2(alpha, x)

```
