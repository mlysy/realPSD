---
title: "**realPSD**: Development Notes"
author: "Martin Lysy, Feiyu Zhu"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{realPSD: Development Notes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\tx}[1]{\textrm{#1}}
\newcommand{\XX}{{\bm{X}}}
\newcommand{\YY}{{\bm{Y}}}
\newcommand{\ZZ}{{\bm{Z}}}
\newcommand{\tth}{{\bm{\theta}}}
\newcommand{\pph}{{\bm{\varphi}}}
\newcommand{\aal}{{\bm{\alpha}}}
\newcommand{\bbe}{{\bm{\beta}}}
\newcommand{\s}{\sigma}
\newcommand{\f}{f}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\psd}{\operatorname{\mathcal S}}
\newcommand{\cov}{\operatorname{cov}}
\newcommand{\var}{\operatorname{var}}
\newcommand{\acf}{\operatorname{\gamma}}
\newcommand{\carfima}{\operatorname{CARFIMA}}
\newcommand{\rv}[3]{#2_{#1},\ldots,#2_{#3}}
\newcommand{\fs}{\f_s}
\newcommand{\kbt}{k_BT}
\newcommand{\Aw}{A_{\tx{w}}}
\newcommand{\Rw}{R_{\tx{w}}}

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
# link to packages
pkg_link <- function(pkg, link) {
  if(link == "github") {
    link <- paste0("https://github.com/mlysy/", pkg)
  } else if(link == "cran") {
    link <- paste0("https://CRAN.R-project.org/package=", pkg)
  }
  paste0("[**", pkg, "**](", link, ")")
}
cran_link <- function(pkg) pkg_link(pkg, "cran")
github_link <- function(pkg) pkg_link(pkg, "github")
```

# Overview

Let $X(t)$ denote a continuous time mean-zero stationary process with autocorrelation $\acf(t) = \cov(X_s, X_{s+t})$. with power spectral density (PSD) defined via the inverse Fourier transform relation
$$
\acf(t) = \int_{-\infty}^\infty e^{-2\pi i t\f} \psd(\f) \, \ud \f,
$$
where $\f \in \mathbb R$ is the frequency in Hertz.  The **realPSD** package provides tools for estimation of parametric PSDs of the form
$$
\psd(\f,\tth) = \s^2 \cdot U(\f, \pph),
$$
where the PSD parameters are $\tth = (\s, \pph)$.   The data $\XX = (\rv 0 X {N-1})$ consist of discrete observations of $X(t)$ recorded at sampling frequency $\fs$, such that $X_n = X(n/\fs)$.  

This package is essentially a port of `r github_link("realSHO")`, but with the option of adding one's own models.  The interface is through the R `r cran_link("TMB")` package, which uses [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD) to enable gradient-based optimization algorithms to efficiently perform parameter estimation.


<!-- This interface is written in Python, and uses [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (AD) to use gradient-based optimization algorithms to efficiently perform parameter estimation. -->

<!-- ~~Currently the interface is through the R `r cran_link("TMB")` package, which makes the underlying C++ code difficult to interface from a different language (e.g., Python, Matlab).  A later step is to create a self-contained C++ library relying only on `r pkg_link("CppAD", "https://coin-or.github.io/CppAD/doc/cppad.htm")`, the C++ automated differentiation (AD) library on top of which **TMB** is built.~~ -->

# Methods

**realPSD** provides three different methods for parameter estimation outlined below.  For each method, estimation is via optimization over a function of $\pph$, with the fitted value of $\s$ being profiled out.  In all cases the estimators are built on the discrete Fourier transform of the data, $\YY = (\rv 1 Y K)$, where $N = 2K + 1$ and
$$
Y_k = \frac 1 N \left\vert \sum_{n=0}^{N-1} e^{-2\pi i k n} X_n\right\vert^2.
$$

## Maximum Likelihood

The maximum likelihood estimator is obtained by maximizing the so-called Whittle loglikelihood function,
\begin{equation}
\label{eq:mle}
\ell_W(\tth \mid \YY) = - \sum_{k=1}^K\left(\frac{Y_k}{\psd_k(\tth)} + \log \psd_k(\tth)\right),
\end{equation}
where $\psd_k(\tth) = \fs \cdot \psd(\f_k, \tth)$ and $f_k = \tfrac k N \fs$.  Note that for given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:mle} is
$$
\hat \s^2(\pph) = \frac 1 K \sum_{k=1}^K \frac{Y_k}{U_k(\pph)},
$$
where $U_k(\pph) = \fs \cdot U(\f_k, \pph)$.  Therefore, upon maximizing the profile likelihood
$$
\begin{aligned}
\ell_{W,\tx{prof}}(\pph \mid \YY) & = \ell_W(\pph, \hat \s(\pph) \mid \YY) \\
& = - K (1 + \log \hat \sigma^2(\pph)) - \sum_{k=1}^K \log U_k(\pph),
\end{aligned}
$$
we obtain the MLE $\hat \tth = (\hat \pph, \hat \s(\hat \pph))$.

## Nonlinear Least-Squares

In many situations with high-throughput (HTP) data, $K$ is very large, such that calculating the MLE becomes very computationally intensive.  An approximation to the MLE involves binning the periodogram ordinates.  That is, let $K = B \cdot N_B$, and
$$
\bar Y_m = \frac 1 B \sum_{k \in I_m} Y_k, \qquad I_m = \{k \in \mathbb N: (m-1)B < k \le mB\}.
$$
Then the nonlinear least-squares (NLS) estimator maximizes the objective function
\begin{equation}
\label{eq:nls}
Q_{\tx{NLS}}(\tth) = - \sum_{m=1}^{N_B} \big(\bar Y_m - \bar{\psd}_m(\tth)\big)^2,
\end{equation}
where $\bar{\psd}_m(\tth) = \fs \cdot \psd(\bar \f_m, \tth)$ and $\bar \f_m = \tfrac 1 B \sum_{k\in I_m} \f_k$.  For given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:nls} is
$$
\hat \s^2(\pph) = \frac{\sum_{m=1}^{N_B} \bar{U}_m(\pph) \bar Y_m }{\sum_{m=1}^{N_B} \bar{U}_m(\pph)^2},
$$
where $\bar{U}_m(\pph) = \fs \cdot U(\bar f_m, \pph)$.  Therefore, upon maximizing the objective function
$$
Q_{\tx{NLS},\tx{prof}}(\pph) = -\sum_{m=1}^{N_B} \big(\bar Y_m - \hat \sigma^2(\pph) \cdot \bar{U}_m(\pph)\big)^2,
$$
we obtain the maximum of \\eqref{eq:nls} via $\hat \tth = (\hat \pph, \hat \sigma(\hat \pph))$.

## Variance-Stabilized Least-Squares

Let $Z_m = \log(\bar Y_m)$.  Then the log-periodogram (LP) estimator maximizes the approximate loglikelihood
\begin{equation}
\label{eq:lp}
\ell_{\tx{LP}}(\tth \mid \YY) = -\frac B 2 \sum_{m=1}^{N_B} \big(Z_m + C_B - \log \bar{\psd}_m(\tth)\big)^2,
\end{equation}
where $C_B = \log B - \psi(B)$ and $\psi(x)$ is the [digamma function](https://en.wikipedia.org/wiki/Digamma_function).  For given $\pph$, the value of $\s^2$ maximizing \\eqref{eq:lp} is
$$
\hat \s^2(\pph) = \exp\left\{\hat \zeta(\pph) + C_B\right\}, \qquad \hat \zeta(\pph) = \frac{1}{N_B} \sum_{m=1}^{N_B} Z_m - \log \bar{U}_m(\pph),
$$
such that upon maximizing
$$
\ell_{\tx{LP},\tx{prof}}(\pph \mid \YY) = - \sum_{m=1}^{N_B} \big(Z_m - \hat \zeta(\pph) - \log \bar U_m(\pph)\big)^2
$$
we obtain the maximum of \\eqref{eq:lp} via $\hat \tth = (\hat \pph, \hat \sigma(\hat \pph))$.

## Variance Estimators

The MLE loglikelihood \\eqref{eq:mle} can be used to calculate the observed Fisher information, which in turn can be used to produce the variance estimates
$$
\widehat{\var}(\hat \tth_{\tx{MLE}}) = -\left[\frac{\partial^2}{\partial \tth^2} \ell_W(\hat\tth_{\tx{MLE}} \mid \YY)\right]^{-1}.
$$
It turns out that the same holds for the LP loglikelihood \\eqref{eq:lp} up to a small inflation factor:
$$
\widehat{\var}(\hat \tth_{\tx{LP}}) = D_B \times -\left[\frac{\partial^2}{\partial \tth^2} \ell_{\tx{LP}}(\hat\tth_{\tx{LP}} \mid \YY)\right]^{-1},
$$
where $D_B = B \psi'(B)$.  Note that $D_{10} = `r round(10 * trigamma(10), 2)`$ and $D_{100} = `r round(100 * trigamma(100), 4)`$, such that correcting by $D_B$ has little impact in practice.

<!-- Both the MLE and LP loglikelihoods \\eqref{eq:mle} and \\eqref{eq:lp} can be used to calculate the observed Fisher information, which in turn can be used to produce the variance estimates -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \widehat{\var}(\hat \tth_{\tx{MLE}}) & = -\left[\frac{\partial^2}{\partial \tth^2} \ell_W(\hat\tth_{\tx{MLE}} \mid \YY)\right]^{-1} \\ -->
<!-- \widehat{\var}(\hat \tth_{\tx{LP}}) & = -\left[\frac{\partial^2}{\partial \tth^2} \ell_{\tx{LP}}(\hat\tth_{\tx{LP}} \mid \YY)\right]^{-1}. -->
<!-- \end{aligned} -->
<!-- $$ -->

In contrast to the likelihood-based estimators, the variance of the NLS estimator is defined by the sandwich formula
\begin{equation}
\label{eq:sand}
\widehat{\var}(\hat \tth_{\tx{NLS}}) = - \bm A^{-1} \bm B \bm A^{-1},
\end{equation}
where for $g_m(\tth) = (\bar Y_m - \bar{\psd}_m(\tth))^2$, we have
$$
\begin{aligned}
\bm A & = \sum_{m=1}^{N_B} \frac{\partial^2}{\partial \tth^2} g_m(\hat \tth_{\tx{NLS}}) \\
\bm B & = \sum_{m=1}^{N_B} \left[\frac{\partial}{\partial \tth} g_m(\hat \tth_{\tx{NLS}})\right]\left[\frac{\partial}{\partial \tth} g_m(\hat \tth_{\tx{NLS}})\right]'.
\end{aligned}
$$

## Model Residuals

Both the NLS and LP (profile) optimization problems can be rewritten as minimizing a sum-of-squares objective function
$$
S(\pph) = \sum_{m=1}^{N_B} r_m(\pph)^2,
$$
for which specialized optimization algorithms such as [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) or [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) can be used.

## Scaling

**TODO:** Better explain why we do this.

For numerical stability, we have found that it is often preferable to express the Whittle likelihood \\eqref{eq:mle} as
$$
\ell_W(\tth \mid \YY) = - \sum_{k=1}^K\left(\frac{Y_k^{(s)}}{\psd_k(\tth)/C_s} + \log \psd_k(\tth)\right),
$$
where $Y_k^{(s)} = Y_k/C_s$ and $C_s > 0$ is a scaling factor (e.g., $C_s = K^{-1} \sum_{k=1}^K Y_k$).  In this case, the value of $\sigma^2$ maximing \\eqref{eq:mle} for given $\pph$ is
$$
\hat \s^2(\pph) = C_s \cdot \hat \s^2_{(s)}(\pph), \qquad \hat \s^2_{(s)}(\pph) = \frac {1} K \sum_{k=1}^K \frac{Y_k^{(s)}}{U_k(\pph)},
$$
and the profile loglikelihood may be expressed as
$$
\ell_{W,\tx{prof}}(\pph \mid \YY) = - K (1 + \log C_s + \log \hat \sigma_{(s)}^2(\pph)) - \sum_{k=1}^K \log U_k(\pph).
$$

Similarly, for the NLS estimator we may maximize the objective function
$$
Q_{\tx{NLS}}^{(s)}(\tth) = - \sum_{m=1}^{N_B} \big(\bar Y_m^{(s)} - \bar{\psd}_m(\tth)/C_s\big)^2,
$$
where $\bar Y_m^{(s)} = \bar Y_m / C_s$.  The corresponding profiled function is
$$
Q_{\tx{NLS},\tx{prof}}^{(s)}(\pph) = -\sum_{m=1}^{N_B} \big(\bar Y_m^{(s)} - \hat \sigma_{(s)}^2(\pph) \cdot \bar{U}_m(\pph)\big)^2,
$$
where
$$
\hat \s_{(s)}^2(\pph) = \frac{\sum_{m=1}^{N_B} \bar{U}_m(\pph) \bar Y_m^{(s)} }{\sum_{m=1}^{N_B} \bar{U}_m(\pph)^2} = \hat \s^2(\pph)/C_s.
$$

Scaling is typically not needed for the LP estimator, but for completeness we provide the calculations here.  That is, for $Z_m^{(s)} = Z_m - C_s$, we have
$$
\ell_{\tx{LP}}(\tth \mid \YY) = - \frac B 2 \big(Z_m^{(s)} + C_s + C_B - \log \bar{\psd}_m(\tth) \big)^2
$$
and
$$
\hat \s^2(\pph) = \exp\left\{\hat \zeta_{(s)}(\pph) + C_s + C_B\right\}, \qquad \hat \zeta_{(s)}(\pph) = \frac{1}{N_B} \sum_{m=1}^{N_B} Z_m^{(s)} - \log \bar{U}_m(\pph).
$$

## Median Binning

The NLS and LP estimators are defined in terms of the bin mean $\bar Y_m$.  To decrease sensitivity to outliers, one might consider the bin median $\tilde Y_m = \operatorname{median}\{Y_k: k \in I_m\}$.  In this case, we have
$$
\begin{aligned}
\lim_{B\to\infty} E[\tilde Y_m] & = \bar{\psd}_m(\tth) \cdot \log 2, \\ \lim_{B\to\infty} E[\log \tilde Y_m] & = \log \bar{\psd}_m(\tth) + \log \log 2,
\end{aligned}
$$
such that we may try to maximize the objective functions
$$
\begin{aligned}
\tilde Q^{(s)}_{\tx{NLS}}(\tth) & = - \sum_{m=1}^{N_B}\big(\tilde Y_m^{(s)} - (\log 2/C_s) \cdot \bar{\psd}_m(\tth)\big)^2 \\
\tilde Q^{(s)}_{\tx{LP}}(\tth) & = - \sum_{m=1}^{N_B} \big(\tilde Z_m^{(s)} + C_s - \log \log 2 - \log \bar{\psd}_m(\tth)\big)^2.
\end{aligned}
$$
Note here that neither the LP nor NLS estimators are asympotically loglikelihoods, such that variance estimators should use the sandwich formula \\eqref{eq:sand}.

# Examples

## Simple Harmonic Oscillator with Noise Floor

The SHOW model is given by
\begin{equation}
\label{eq:show}
\begin{aligned}
\psd(\f, \tth) & = \Aw + \frac{\kbt/(k \cdot \pi \f_0 Q)}{[(\f/\f_0)^2-1]^2 + [f/(\f_0Q)]^2} \\
& = \sigma^2 \times \left\{\Rw + \frac{1}{[(\f/\f_0)^2-1]^2 + [f/\gamma]^2}\right\},
\end{aligned}
\end{equation}
where $\sigma^2 = \kbt/(k\cdot \pi \f_0Q)$, $\Rw = \Aw/\sigma^2$, $\gamma = \f_0Q$, and $\pph = (\f_0, \gamma, \Rw)$.

## Fractional Ornstein-Uhlenbeck Model

The fOU model is expressed as
$$
\ud X(t) = -\gamma X(t)\, \ud t + \tau \, \ud B_t^H,
$$
where $\gamma, \tau > 0$ and $B_t^H$ is fractional Brownian motion with Hurst parameter $0 < H < 1$.  The PSD for this model is
\begin{equation}
\label{eq:fcar}
\psd(\f, \tth) = \sigma^2 \times \frac{|\f|^{1-2H}}{\f^2 + \gamma^2},
\end{equation}
where $\pph = (H, \gamma)$ and $\sigma^2 = \tau^2 \cdot \Gamma(2H+1)\sin(\pi H)/(2\pi)$.

## CARFIMA Models

More generally, a $\carfima(p,q)$ process is defined by the differential equation
$$
X_t^{(p)} = + \sum_{k=1}^{p-1} \alpha_k X_t^{(k-1)} + \tau \left[B_t^{H(1)} + \sum_{m=1}^{q} \beta_m B_t^{H(m+1)}\right],
$$
where $X(t)$ is stationary if and only if
$$
\alpha(x) = x^p - \sum_{k=1}^{p-1} \alpha_k x^{k-1} = \prod_{k=1}^p(x - r_k)
$$
is such that $\mathscr R(r_k) < 0$ for $k = 1,\ldots,p$. The PSD of the model is given by
\begin{equation}
\label{eq:carfima}
\psd(\f, \tth) = \sigma^2 \times \frac{|\f|^{1-2H} \cdot |1 + \sum_{m=1}^q \beta_m (i\f)^m|^2}{|(i\f)^p - \sum_{k=1}^p \alpha_k (i\f)^{k-1}|^2}, 
\end{equation}
where $\pph = (H, \aal, \bbe)$, and $\sigma^2 = \tau^2 \cdot \Gamma(2H+1)\sin(\pi H)/(2\pi)$.  
<!-- , $0 < H < 1$, and $\aal = (\rv 1 \alpha q )$ and $\bbe = (\rv 1 \beta p )$ are such that the complex polynomials in the numerator and denominator of \\eqref{eq:carfima} have no roots in the unit circle.  The model parameters are $\pph = (\aal, \bbe, H)$. -->
Since **TMB** does not support complex arithmetic, the polynomials are calculated as follows.

Consider a polynomial $p(x) = \sum_{k=0}^p \alpha_k x^k$ with $\alpha_k \in \mathbb R$.
\begin{align*}
p(i\f) 
	& = \sum_{k=0}^p \alpha_k (i\f)^k \\
	& = \sum_{k=0}^{q} \alpha_{2k} (i\f)^{2k} + \sum_{k=0}^r \alpha_{2k+1} (i\f)^{2k+1} \\
	& = \sum_{k=0}^q (-1)^k \alpha_{2k} f^{2k} + i\f \sum_{k=0}^r (-1)^k \alpha_{2k+1} f^{2k}
\end{align*}


<!-- where $g_m(\tth) = (\bar Y_m - \bar{\psd}_m(\tth))^2$. -->

<!-- ## Implementation -->

<!-- ### Python -->

<!-- I'm not sure about OOP in Python, but in pseudo C++ I suggest we have a base class for each PSD model defined by $U(\f, \pph)$, and derived class for each method.  So for example: -->

<!-- ```{Rcpp, eval = FALSE} -->
<!-- class showModel { -->
<!-- // private/protected members here -->
<!-- public: -->
<!--   double UFun(const double f, const Vector& phi);   -->
<!-- }; -->

<!-- template <class Model> -->
<!-- class LPFit : public Model { -->
<!-- public: -->
<!--   // constructor: copy + allocate memory here -->
<!--   LPFit(const Vector& xPSD, const Vector& yPSD, const int binSize); -->
<!--   // conditional optimum of log_tau = log(tau) = log(sigma^2) -->
<!--   double logTauFit(const Vector& phi);		  -->
<!--   // objective function: set up to minimize it -->
<!--   double obj(const Vector& phi); -->
<!--   // gradient & hessian -->
<!--   void grad(Vector& g, const Vector& phi); -->
<!--   void hess(Matrix& H, const Vector& phi); -->
<!--   // sometimes these re-use calculations from OFun. -->
<!--   // In that case, it's more efficient to have something like -->
<!--   double obj(Vector& g, Matrix& H, const Vector& phi); -->
<!--   // default optimization method. -->
<!--   //user has the option to do this manually with `obj`, `grad`, `hess` if -->
<!--   // default doesn't work. -->
<!--   // Or, simply don't provide a default if it's too complicated! -->
<!--   void fit(Vector& phi, double& logTau, const Vector& phiInit, -->
<!--            ...); // ellipsis is for additional tuning parameters -->
<!--   // variance estimator -->
<!--   void var(Matrix& V, const Vector& phi, const double logTau); -->
<!-- }; -->
<!-- ``` -->

<!-- Some useful Python packages: -->

<!-- - [**autograd**](https://github.com/HIPS/autograd): Automatic differentiation of Python and Numpy functions. -->
<!-- - [**NLopt**](https://nlopt.readthedocs.io/en/latest/): Excellent nonlinear optimization library, with Python interface.  I suggest using gradient-based algorithms in combination with the above. -->

<!-- Other functions the library should contain: -->

<!-- - `tsSim`: Simulate time series with given PSD. -->
<!-- - `periodogram`: Calculate the periodogram of a time series. -->
<!-- - `fisherGstat`: For denoising. -->
